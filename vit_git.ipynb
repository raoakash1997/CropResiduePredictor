{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device : cuda:0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset processing\n",
    "class CropDataloader(Dataset):\n",
    "\n",
    "    def __init__(self, image_directory, mask_directory, transform = None ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Take .jpg as original and .tif as label images\n",
    "\n",
    "        self.images = sorted(glob.glob(os.path.join(image_directory, \"*.jpg\")))\n",
    "\n",
    "        self.masks = sorted(glob.glob(os.path.join(mask_directory, \"*.tif\")))\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        original_image = Image.open(self.images[index]).convert('L')\n",
    "\n",
    "        mask_image = Image.open(self.masks[index])\n",
    "\n",
    "        # Transform into the tensors\n",
    "\n",
    "        tensor_image = self.transform(original_image)\n",
    "        tensor_mask = self.transform(mask_image)\n",
    "\n",
    "        return tensor_image, tensor_mask\n",
    "\n",
    "\n",
    "\n",
    "# Image path\n",
    "image_path = \"/local/data/sdahal_p/Crop/data/train/original/\"\n",
    "\n",
    "mask_image_path = \"/local/data/sdahal_p/Crop/data/train/mask/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataloader for images\n",
    "\n",
    "dataset = CropDataloader(image_path, mask_image_path, transform = transform)\n",
    "dataloader = DataLoader(dataset, batch_size = 1, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "# Patch embedding\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self,model_dim, patch_size, number_of_patches, in_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patcher = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels = in_channels,\n",
    "                out_channels = model_dim,\n",
    "                kernel_size = patch_size,\n",
    "                stride = patch_size,\n",
    "            ),                  \n",
    "            nn.Flatten(2))\n",
    "        \n",
    "        self.batch_size = 1\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(size=(self.batch_size, in_channels, model_dim)), requires_grad=True)\n",
    "\n",
    "        # self.position_embeddings = nn.Parameter(torch.randn(size=(self.batch_size, number_of_patches + 1, model_dim)), requires_grad=True)\n",
    "\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(size=(self.batch_size, number_of_patches, model_dim)), requires_grad=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.05)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # cls token\n",
    "\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "\n",
    "        x = self.patcher(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # x = torch.cat([cls_token, x], dim=1)\n",
    "        \n",
    "        x = self.position_embeddings + x \n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropFormer(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim, patch_size, number_of_patches, in_channels, encoders, num_heads, num_classes, original_img_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings_block = PatchEmbedding(model_dim, patch_size, number_of_patches, in_channels)\n",
    "\n",
    "        encoder_layer= nn.TransformerEncoderLayer(d_model = model_dim, nhead = num_heads)\n",
    "\n",
    "        self.encoder_blocks = torch.nn.TransformerEncoder(encoder_layer , num_layers = encoders)\n",
    "\n",
    "        self.head = nn.Linear(model_dim, patch_size * patch_size)\n",
    "\n",
    "\n",
    "        # self.linear_classifier = torch.nn.Linear(in_features=model_dim, out_features=num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x =  self.embeddings_block(x)\n",
    "\n",
    "        # print(x.size())\n",
    "\n",
    "        x = self.encoder_blocks(x)\n",
    "\n",
    "        # x = self.linear_classifier(x[:, 0, :])\n",
    "\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x.view(batch_size, in_channels, original_img_size, original_img_size)\n",
    "\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_dim = 512\n",
    "patch_size = 32\n",
    "number_of_patches = 256\n",
    "in_channels = 1\n",
    "\n",
    "encoders = 6\n",
    "\n",
    "num_heads = 8\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "original_img_size = 512\n",
    "\n",
    "crop_model = CropFormer(model_dim, patch_size, number_of_patches, in_channels, encoders, num_heads, num_classes, original_img_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(crop_model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8123\n",
      "Epoch [2/10], Loss: 0.4967\n",
      "Epoch [3/10], Loss: 0.6087\n",
      "Epoch [4/10], Loss: 0.4717\n",
      "Epoch [5/10], Loss: 0.5204\n",
      "Epoch [6/10], Loss: 0.5018\n",
      "Epoch [7/10], Loss: 0.3711\n",
      "Epoch [8/10], Loss: 0.5843\n",
      "Epoch [9/10], Loss: 0.4838\n",
      "Epoch [10/10], Loss: 0.2800\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model, loss, and optimizer\n",
    "\n",
    "# Training loop\n",
    "\n",
    "crop_model.train(True)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(inputs.size())\n",
    "\n",
    "        outputs = crop_model(inputs)\n",
    "\n",
    "        # print(outputs.size())\n",
    "\n",
    "        # print(targets.size())\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
